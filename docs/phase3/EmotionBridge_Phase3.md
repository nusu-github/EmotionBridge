# EmotionBridge Phase 3 提案書（v1.0）
>
> **ステータス**: 検証完了・実装着手可  
> **作成日**: 2026-02-17  
> **前提**: Phase 0–2 完了済み、V-01/V-02/V-03 検証完了済み  
> **旧版からの変更**: SER ベース設計を全面廃止し、JVNV 韻律プロファイル直接マッチング方式に転換
---

## 0. エグゼクティブサマリ

EmotionBridge は、テキストの感情を VOICEVOX の韻律パラメータに自動変換するシステムである。
旧設計（v0.1–v0.3）では、音声感情認識モデル（SER: emotion2vec+）を「審判」としてパラメータの良し悪しを評価する方式を採用していた。しかし以下の構造的問題が蓄積し、Phase 3 着手前にアーキテクチャの全面的な見直しを行った。

- Phase 0 テキスト感情エンコーダの trust 相関が 0.36（No-Go 判定）
- emotion2vec+ の合成音声ドメインギャップが未検証かつ高リスク
- 暫定スコア（weak teacher）の上に triplet loss → パラメータ回帰、というノイズの多段積み
新設計では、**JVNV 感情音声コーパス**（人間が 94% の精度で感情を識別できるラベル付き音声）から韻律特徴量を物理的に抽出し、それを正解信号とする。検証実験（V-01/V-02/V-03）により、この方式が実用的であることを確認した。

---

## 1. 設計転換の経緯と根拠

### 1.1 旧設計の構造的問題

```
旧設計のノイズ連鎖:
  テキスト → BERT感情分析(trust相関0.36) → 8D感情ベクトル(ノイジー)
    → emotion2vec+スコア(weak teacher, ドメインギャップ未検証)
      → triplet loss(ノイズ教師で学習)
        → パラメータ回帰(ノイズの上にノイズ)
```

各段階の不確実性が掛け算で累積し、最終出力の信頼度が構造的に担保できない状態だった。

### 1.2 転換の着想

E2E 音声合成（GPT-4o、Moshi、Mini-Omni 等）の技術調査を通じて、「感情を感情で測る」SER ベースの評価ループではなく、「韻律の物理量で直接比較する」アプローチの優位性が明らかになった。
ただし、E2E モデルを Voice として使用することは本プロジェクトの目的に合致しない。EmotionBridge の目的は「VOICEVOX・CeVIO 等の既存キャラクター音声合成の韻律を自動調整する」ことであり、特定のキャラクターの声で感情表現を実現するには、パラメータ制御方式が唯一の現実解である。

### 1.3 新設計の核心

```
新設計:
  JVNV感情音声(1,615件, 6感情, 94%認識率)
    → eGeMAPS韻律特徴抽出(88D, 物理量)
      → 感情別韻律プロファイル(正解信号)
  Phase 1 VOICEVOXグリッドサーチ音声(25,416件)
    → eGeMAPS韻律特徴抽出(88D)
      → 韻律空間上で最近傍マッチング
        → 感情ごとの推奨パラメータ
```

SER の代わりに JVNV 実測韻律、感情埋め込み空間の代わりに eGeMAPS 物理量空間。間接的な推定が直接的な計測に置き換わることで、ノイズの連鎖が断ち切られる
---

## 2. 検証実験の結果

### 2.1 V-01: JVNV 感情分離性

| 指標                                   | 結果  | 判定                        |
| -------------------------------------- | ----- | --------------------------- |
| 全体 silhouette（88D）                 | 0.009 | 6感情クラスタとしては不成立 |
| ペアワイズ最大（anger–sad, Tier1 11D） | 0.284 | 強い分離                    |
| ペアワイズ（anger–fear）               | 0.263 | 強い分離                    |
| ペアワイズ（happy–sad）                | 0.195 | 有意な分離                  |
| ペアワイズ最小（fear–surprise）        | 0.026 | 分離なし                    |
| Arousal 回帰 R²                        | 0.345 | 閾値 0.30 達成              |
| Valence 回帰 R²                        | 0.193 | 閾値 0.15 達成              |
**判定: Conditional Go** — 6感情の離散クラスタリングは不成立だが、韻律空間に感情情報は存在する。anger / happy / sad が韻律的に明確に区別可能で、fear / disgust / surprise は中間的。この構造に合わせた設計（カテゴリ直接マッチング）に転換。

### 2.2 V-02: VOICEVOX 韻律応答性

| 制御軸       | 最も応答する韻律特徴   | 偏相関 | 応答判定 |
| ------------ | ---------------------- | ------ | -------- |
| pitch_shift  | F0 percentile 80       | 0.982  | Go       |
| pitch_range  | F0 pctlrange 0-2       | 0.615  | Go       |
| speed        | loudnessPeaksPerSec    | 0.775  | Go       |
| energy       | spectralFlux V amean   | 0.778  | Go       |
| pause_weight | loudness percentile 20 | 0.413  | Go       |
**判定: Go（Feature gate）** — VOICEVOX の 5D 制御パラメータは韻律特徴空間上で系統的な変動を生じさせる。A/V direction gate は arousal/valence 経由の設計廃止に伴い、診断指標として留置。

### 2.3 V-03: ドメインギャップ・感情マッチング

| 指標                     | 正規化前 | 正規化後 |
| ------------------------ | -------- | -------- |
| Wasserstein 距離（平均） | 15.42    | 0.15     |
| MMD                      | 0.255    | 0.005    |
| Overlap ratio            | —        | 0.72     |
**感情マッチング結果（推奨パラメータ）:**
| 感情         | pitch_shift | pitch_range | speed     | energy    | pause_weight |
| ------------ | ----------- | ----------- | --------- | --------- | ------------ |
| **anger**    | -0.09       | **+0.65**   | +0.07     | **+0.53** | +0.03        |
| **happy**    | **+0.22**   | +0.19       | **+0.26** | -0.32     | +0.16        |
| **sad**      | **-0.31**   | -0.02       | **-0.22** | **-0.84** | +0.30        |
| **disgust**  | -0.33       | +0.12       | -0.29     | -0.52     | -0.22        |
| **fear**     | +0.19       | -0.05       | +0.23     | -0.41     | **+0.45**    |
| **surprise** | +0.09       | -0.01       | +0.26     | -0.38     | **+0.42**    |
これらのパターンは感情韻律研究の文献知見と整合する:

- **anger**: energy 最大・pitch_range 最大（声を張り、抑揚を大きく）
- **happy**: pitch_shift 最大・speed 正（声が高く、速い）
- **sad**: energy 最小（-0.84, std=0.13 で極めてタイト）、全体的に低く遅い
- **fear**: pitch_shift 正・energy 負・pause_weight 最大級（声が上ずるが小声で言い淀む）
- **disgust**: pause_weight が唯一負（嫌悪を吐き捨てるように間を詰める）
**判定: Go** — 正規化によりドメインギャップを実用的レベルまで縮小。感情マッチングが意味のあるパラメータパターンを返すことを確認。

### 2.4 聴取確認

推奨パラメータで VOICEVOX 音声を生成し、デフォルト（ニュートラル）と比較聴取を実施。感情方向への変化は確認でき、デフォルトよりは確実にマシ。劇的な変化ではないが、5D 韻律パラメータ制御の範囲内では妥当な結果。スタイル混合による追加制御で更なる品質向上の余地あり
---

## 3. 新アーキテクチャ

### 3.1 全体パイプライン

```
[オフライン: 教師データ構築]
JVNV (1,615件, 6感情, CC BY-SA 4.0)
  → NV区間除外（非言語発話ラベルで除外）
  → openSMILE eGeMAPSv02 抽出 (88D)
  → 話者内 z-score 正規化
  → 感情別韻律プロファイル（重心 + 分布）
Phase 1 VOICEVOX グリッドサーチ音声 (25,416件)
  → openSMILE eGeMAPSv02 抽出 (88D)
  → キャラクタ/スタイル内 z-score 正規化
  → 各サンプルに「各感情プロファイルとの韻律距離」を付与
[学習]
入力: テキスト感情カテゴリ（6クラス）
教師: 韻律距離に基づく上位 k 件の制御パラメータ
出力: 5D 制御パラメータ + スタイル選択（後述）
[推論]
テキスト → 感情分類 → パラメータ生成器 → 5D + スタイル → VOICEVOX → 音声
```

### 3.2 旧設計との対比

| 項目             | 旧設計（v0.3）                       | 新設計（v1.0）                      |
| ---------------- | ------------------------------------ | ----------------------------------- |
| 正解信号         | emotion2vec+ スコア（weak teacher）  | JVNV 韻律プロファイル（物理量）     |
| 比較空間         | 感情埋め込み空間（ブラックボックス） | eGeMAPS 韻律特徴空間（解釈可能）    |
| 感情表現         | 8D Plutchik 連続値                   | 6 感情カテゴリ                      |
| 中間層           | 4段（感情分析→SER→ランキング→回帰）  | 2段（感情分類→韻律マッチング→回帰） |
| ドメインギャップ | emotion2vec+（未検証・高リスク）     | eGeMAPS（正規化後 overlap 72%）     |
| 評価基盤         | SER スコア（循環的）                 | 韻律距離（物理量ベース）            |

### 3.3 制御空間

5D 制御パラメータの定義は旧設計から継続。各パラメータは -1.0 〜 +1.0 の範囲。

| パラメータ   | 音声学的意味        | V-02 で確認された韻律応答               |
| ------------ | ------------------- | --------------------------------------- |
| pitch_shift  | F0 の平均的な高低   | F0 percentile 80 との偏相関 0.982       |
| pitch_range  | F0 の変動幅（抑揚） | F0 pctlrange との偏相関 0.615           |
| speed        | 発話速度            | loudnessPeaksPerSec との偏相関 0.775    |
| energy       | 音響エネルギー      | spectralFlux V との偏相関 0.778         |
| pause_weight | 休止の長さ          | loudness percentile 20 との偏相関 0.413 |

### 3.4 スタイル制御の拡張

VOICEVOX はキャラクターごとに複数の音声スタイルを提供する。韻律パラメータ 5D に加え、スタイル選択を第 6 の制御軸として導入する。
**ずんだもん（主要ターゲット）のスタイル:**

| スタイル | ID  | 想定される感情的用途 |
| -------- | --- | -------------------- |
| ノーマル | 3   | ニュートラル・基本   |
| あまあま | 1   | 喜び・親しみ         |
| ツンツン | 7   | 怒り・不満           |
| セクシー | 5   | —                    |
| ささやき | 22  | 恐怖・悲しみ（弱）   |
| ヒソヒソ | 38  | 恐怖・秘密           |
| ヘロヘロ | 75  | 疲労・悲しみ         |
| なみだめ | 76  | 悲しみ（強）         |
**四国めたん（副ターゲット）のスタイル:**
| スタイル | ID  | 想定される感情的用途 |
| -------- | --- | -------------------- |
| ノーマル | 2   | ニュートラル・基本   |
| あまあま | 0   | 喜び・親しみ         |
| ツンツン | 6   | 怒り・不満           |
| セクシー | 4   | —                    |
| ささやき | 36  | 恐怖・悲しみ（弱）   |
| ヒソヒソ | 37  | 恐怖・秘密           |
**スタイル制御の方針:**
Phase 3 初期実装では、感情カテゴリとスタイルのマッピングをルールベースで定義する。各スタイルの eGeMAPS 特徴量を抽出・正規化し、JVNV 感情プロファイルとの韻律距離が最小のスタイルを各感情のデフォルトとする。将来的には、スタイル混合比率も学習対象とする拡張が可能。

---

## 4. Phase 0 の再設計

### 4.1 変更点

旧設計では 8D Plutchik 連続ベクトル（Softmax 確率分布）を出力していた。新設計では 6 感情カテゴリ分類に切り替える。

| 項目               | 旧 Phase 0                                                        | 新 Phase 0                                 |
| ------------------ | ----------------------------------------------------------------- | ------------------------------------------ |
| 出力               | 8D Softmax 確率（Plutchik 全感情）                                | 6 クラス分類確率                           |
| 感情               | joy, sadness, anticipation, surprise, anger, fear, disgust, trust | anger, disgust, fear, happy, sad, surprise |
| 訓練データ         | WRIME（8感情強度）                                                | WRIME（6感情にマッピング）                 |
| No-Go だった trust | 含む（相関 0.36 で足を引っ張っていた）                            | 除外                                       |
| anticipation       | 含む                                                              | 除外（JVNV に対応なし）                    |

### 4.2 WRIME → JVNV 感情マッピング

| WRIME（Plutchik） | JVNV     | 対応     |
| ----------------- | -------- | -------- |
| joy               | happy    | 直接対応 |
| sadness           | sad      | 直接対応 |
| anger             | anger    | 直接対応 |
| fear              | fear     | 直接対応 |
| disgust           | disgust  | 直接対応 |
| surprise          | surprise | 直接対応 |
| anticipation      | —        | 除外     |
| trust             | —        | 除外     |
旧 Phase 0 が No-Go だった主因（trust の相関 0.36）は、新設計では自然に解消される。

### 4.3 実装方針

既存の BERT モデル（tohoku-nlp/bert-base-japanese-whole-word-masking）をベースに、6 クラス分類ヘッドで再学習する。旧 Phase 0 の 8D 出力から 6D を抽出して後処理で変換する方式も検討したが、不要な感情軸の干渉を排除するため、再学習を推奨する
---

## 5. 韻律特徴量の設計

### 5.1 特徴量セット

openSMILE eGeMAPSv02（88 パラメータ）をフルセットで使用する。
V-01 の検証では Tier1 サブセット（11D）での全体 silhouette は改善しなかったが、ペアワイズ分離度は向上した。88D フルセットは情報の冗長性を含むが、マッチング時の距離計算においてはノイズ次元の影響が平均化されるため、フルセットを維持する。将来的に特徴量選択による最適化の余地あり。

### 5.2 正規化手法

**セミトーン変換（eGeMAPS 内蔵）+ 話者/キャラクタ内 z-score 正規化**

- F0 関連: eGeMAPS が半音スケール（27.5Hz 基準）で出力するため、追加変換不要
- 全特徴: グループ（話者 or style_id）内で z-score 正規化
- JVNV: 話者 4 名ごとに全感情発話から μ, σ 算出
- VOICEVOX: style_id ごとに全グリッドサーチ音声から μ, σ 算出
V-03 で正規化の有効性を確認済み（Wasserstein: 15.42 → 0.15）。

### 5.3 距離関数

ユークリッド距離を使用。正規化後の 88D 空間で、JVNV 感情プロファイル重心と VOICEVOX 音声間の距離を計算する。V-03 の感情マッチングはこの距離関数で実施し、文献整合的な結果を得た。

### 5.4 NV 区間の扱い

JVNV の非言語発話（笑い声、泣き声等）の時間区間を NV ラベルに基づき除外。除外方法は該当区間の無音化（切り詰めではなく、時間構造を保持）
---

## 6. 学習パイプライン

### 6.1 教師データの構築

```
Step 1: JVNV 感情別韻律プロファイル
  各感情 e ∈ {anger, disgust, fear, happy, sad, surprise} について:
    centroid_e = mean(normalized_egemaps[emotion == e])  # 88D 重心
Step 2: VOICEVOX 音声への韻律距離付与
  各 VOICEVOX サンプル i について:
    各感情 e について:
      distance_ie = euclidean(normalized_egemaps_i, centroid_e)
Step 3: 感情別の推奨パラメータ候補
  各感情 e について:
    韻律距離の上位 k 件（最近傍）の制御パラメータを教師候補とする
```

### 6.2 パラメータ生成器の学習

**入力**: テキスト感情確率（6D、Phase 0 出力）
**教師**: 各感情の上位 k 件の制御パラメータの中央値（外れ値に頑健）
**出力**: 5D 制御パラメータ + スタイル選択
**損失関数**: 重み付き MSE。テキスト感情確率の各成分を重みとして、各感情の推奨パラメータとの加重平均二乗誤差を最小化する。

```
loss = Σ_e  p(e|text) * MSE(predicted_params, recommended_params_e)
```

これにより、「怒り 0.7、驚き 0.3」のようなテキストに対して、anger と surprise の推奨パラメータの中間的な値が学習される。
**アーキテクチャ**:

```
g: Linear(6, 64) → ReLU → Dropout(0.3) → Linear(64, 5) → tanh
```

出力に tanh を適用し [-1, +1] に制約。入力が 6D と小さいため、モデルは極めて軽量。

### 6.3 スタイル選択の学習

パラメータ生成器とは独立に、感情カテゴリ → スタイル ID のマッピングを構築する。
**Phase 3 初期**: ルールベース（各スタイルの eGeMAPS プロファイルと JVNV 感情プロファイルの距離で決定）
**Phase 3 拡張**: スタイル混合比率を含む学習ベースの選択
---

## 7. 評価計画

### 7.1 定量評価

| 指標                     | 定義                                                                    | 目標                                       |
| ------------------------ | ----------------------------------------------------------------------- | ------------------------------------------ |
| ラウンドトリップ韻律距離 | 予測パラメータで生成した音声の eGeMAPS と JVNV 感情プロファイルとの距離 | V-03 のマッチング距離（mean 3.84）を下回る |
| ~~感情分類一致率~~       | ~~生成音声を感情分類器に通し、元のテキスト感情と一致するか~~            | **廃止**: Phase 0 はテキスト分類器であり SER ではない |
| パラメータ予測 MAE       | 推奨パラメータとの平均絶対誤差                                          | 各軸 0.2 以内                              |

### 7.2 主観評価

| 評価項目       | 方法                                                   |
| -------------- | ------------------------------------------------------ |
| A/B テスト     | デフォルト韻律 vs EmotionBridge 韻律、感情適合度の比較 |
| 5段階 MOS      | 感情表現の自然さ（1: 全く不自然 〜 5: 完全に自然）     |
| 感情識別テスト | 音声を聞いて感情を回答、正解率を測定                   |
主観評価は Phase 3 の最終段階で実施。少人数（5–10 名）のパイロット評価を先行し、大規模評価の実施判断を行う。

---

## 8. 既知の制約と将来課題

### 8.1 韻律パラメータ制御の限界

VOICEVOX の 5D 制御パラメータによる感情表現は、パラメータ範囲と制御粒度に制約される。聴取確認では「ニュートラルよりは確実にマシだが、劇的な変化ではない」という評価。スタイル選択との組み合わせ、およびモーラレベルの韻律制御（vowel_length, consonant_length）の追加により、更なる品質向上が期待できる。

### 8.2 6 感情の韻律分離の非均一性

V-01 で確認された通り、韻律空間上での感情分離は非均一:

- **明確に分離**: anger, happy, sad
- **中程度**: disgust
- **分離が弱い**: fear, surprise（互いにほぼ重なる）
fear と surprise の韻律制御は精度が限定的になる見込み。スタイル選択（ささやき、ヒソヒソ等）でカバーする戦略が現実的。

### 8.3 テキスト多様性の制約

Phase 1 のグリッドサーチは約 200 テキストに対して実施されている。未知テキストへの汎化は、パラメータ生成器の学習で確認が必要。テキスト感情分類（Phase 0）の精度が汎化のボトルネックとなる可能性がある。

### 8.4 演技音声を正解とすることの含意

JVNV は演技音声であり、感情の「ステレオタイプ」を反映している可能性がある。ただし JVNV の 94% 認識率は、これらのパターンが日本語話者にとって自然で認識可能であることを示している。EmotionBridge の目的は「人間が感情として認識できる音声」の生成であり、演技音声を正解とすることは合目的的である
---

## 9. 配布方針

### 9.1 公開するもの

- 実験コード（韻律抽出、正規化、マッチング、学習、推論の全スクリプト）
- 設計文書・調査報告書
- 手順書（環境構築、データ準備、実行手順）

### 9.2 公開しないもの

- JVNV から抽出した韻律特徴量データ
- Phase 1 グリッドサーチ音声および抽出済み特徴量
- 学習済みモデルの重み
- 正規化パラメータ、スコアテーブル等の中間成果物

### 9.3 設計原理

利用者は以下のリソースを自ら準備して初めてパイプラインを動かすことができる:

1. JVNV コーパスのダウンロード（CC BY-SA 4.0）
2. VOICEVOX エンジンのローカル起動
3. グリッドサーチ音声の自己生成（Phase 1 パイプライン実行）
4. 韻律特徴量の自己抽出
コードは動作するが、データは自分で作る。この構造により、手順を理解し実行できる開発者のみが利用可能となる。音声合成の自動化を安易に悪用する用途（無断での大量コンテンツ生成等）への技術的フィルターとして機能する。

---

## 10. ライセンス考慮事項

| リソース  | ライセンス                   | 影響                                                                                            |
| --------- | ---------------------------- | ----------------------------------------------------------------------------------------------- |
| JVNV      | CC BY-SA 4.0                 | 商用利用可。派生物も CC BY-SA。再配布しなければ制約なし                                         |
| WRIME     | 研究用途のみ                 | Phase 0 の学習データ。商用利用時は要確認。学習済み重みの配布可否も要確認                        |
| openSMILE | 研究無償・商用要ライセンス   | 研究フェーズでは問題なし。商用化時は librosa + pyworld による代替実装、または商用ライセンス取得 |
| VOICEVOX  | 各キャラクター利用規約に準拠 | キャラクターごとに商用利用条件が異なる。確認が必要                                              |

---

## 11. 開発ロードマップ

```
[Phase 3a: 基盤構築]
  ├── Phase 0 再学習（8D → 6クラス分類への切り替え）
  ├── スタイル別 eGeMAPS プロファイル構築
  └── 感情-スタイルマッピング（ルールベース）
[Phase 3b: パラメータ生成器]
  ├── 教師データ構築（韻律距離ベース）
  ├── 生成器の学習
  ├── ラウンドトリップ評価
  └── ハイパーパラメータ調整
[Phase 3c: スタイル統合]
  ├── 5D パラメータ + スタイル選択の統合推論
  ├── スタイル混合の実験（任意）
  └── 定量評価
[Phase 3d: 主観評価・最終調整]
  ├── パイロット聴取評価（5–10名）
  ├── 評価結果に基づく調整
  └── ドキュメント整備・コード公開準備
[Phase 4: 拡張（将来）]
  ├── 他 TTS へのキャリブレーション（CeVIO, Style-Bert-VITS2 等）
  ├── モーラレベル韻律制御
  ├── E2E モデル音声による教師データ補完
  └── 感情強度の連続制御
```

---

## 12. 参考資料

### データセット・コーパス

- **JVNV**: 日本語感情音声コーパス（4話者, 6感情, 3.94時間, CC BY-SA 4.0）
  - Xin et al., "JVNV: A Corpus of Japanese Emotional Speech with Verbal Content and Nonverbal Vocalizations" (IEEE Access, 2024)
- **WRIME**: 日本語テキスト感情コーパス（43,200件, Plutchik 8感情）

### 韻律特徴量

- **eGeMAPS**: Eyben et al., "The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice Research and Affective Computing" (IEEE Trans. Affective Computing, 2016) — 被引用数 2,346
- **感情韻律サーベイ**: Larrouy-Maestri et al., "The Sound of Emotional Prosody: Nearly 3 Decades of Research" (Perspectives on Psychological Science, 2024)

### ツール

- **openSMILE**: 韻律特徴量抽出（eGeMAPSv02 設定）
- **VOICEVOX**: オープンソース日本語 TTS
